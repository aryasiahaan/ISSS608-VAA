{
  "hash": "7c5139dfa9ffa5261961e483eb098a3a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hands-on Exercise 5\"\nauthor: \"Arya Siahaan\"\ndate: \"May 8, 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\n# **29 Visualising and Analysing Text Data with R: tidytext methods**\n\n## **29.1 Learning Outcome**\n\nIn this hands-on exercise, you will learn how to visualise and analyse text data using R.\n\nBy the end of this hands-on exercise, you will be able to:\n\n-   understand tidytext framework for processing, analysing and visualising text data,\n\n-   write function for importing multiple files into R,\n\n-   combine multiple files into a single data frame,\n\n-   clean and wrangle text data by using tidyverse approach,\n\n-   visualise words with Word Cloud,\n\n-   compute term frequency–inverse document frequency (TF-IDF) using tidytext method, and\n\n-   visualising texts and terms relationship.\n\n## **29.2 Getting Started**\n\n### **29.2.1 Installing and launching R packages**\n\nIn this hands-on exercise, the following R packages for handling, processing, wrangling, analysing and visualising text data will be used:\n\n-   tidytext, tidyverse (mainly readr, purrr, stringr, ggplot2)\n\n-   widyr,\n\n-   wordcloud and ggwordcloud,\n\n-   textplot (required igraph, tidygraph and ggraph, )\n\n-   DT,\n\n-   lubridate and hms.\n\nThe code chunk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,\ntidyverse, tidygraph, ggraph, igraph)\n```\n:::\n\n\n## **29.3 Importing Multiple Text Files from Multiple Folders**\n\n### **29.3.1 Creating a folder list**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnews20 <- \"data/20news/\"\n```\n:::\n\n\n### **29.3.2 Define a function to read all files from a folder into a data frame**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_folder <- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %>%\n    mutate(text = map(file, \n                      read_lines)) %>%\n    transmute(id = basename(file), \n              text) %>%\n    unnest(text)\n}\n```\n:::\n\n\n## **29.4 Importing Multiple Text Files from Multiple Folders**\n\n### **29.4.1 Reading in all the messages from the 20news folder**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_text <- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %>%\n  mutate(folder_out = map(folder, \n                          read_folder)) %>%\n  unnest(cols = c(folder_out)) %>%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/rds/news20.rds\")\n```\n:::\n\n\n## **29.5 Initial EDA**\n\nFigure below shows the frequency of messages by newsgroup.\n\nThe code chunk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_text <- read_rds(\"data/rds/news20.rds\")\nraw_text %>%\n  group_by(newsgroup) %>%\n  summarize(messages = n_distinct(id)) %>%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex05_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## **29.6 Introducing tidytext**\n\n-   Using tidy data principles in processing, analysing and visualising text data.\n\n-   Much of the infrastructure needed for text mining with tidy data frames already exists in packages like ‘dplyr’, ‘broom’, ‘tidyr’, and ‘ggplot2’.\n\nFigure below shows the workflow using tidytext approach for processing and visualising text data.\n\n![](images/clipboard-71002374.png)\n\n### **29.6.1 Removing header and automated email signitures**\n\nNotice that each message has some structure and extra text that we don’t want to include in our analysis. For example, every message has a header, containing field such as “from:” or “in_reply_to:” that describe the message. Some also have automated email signatures, which occur after a line like “–”.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_text <- raw_text %>%\n  group_by(newsgroup, id) %>%\n  filter(cumsum(text == \"\") > 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %>%\n  ungroup()\n```\n:::\n\n\n::: {.callout-note title=\"Things to learn from the code chunk:\"}\n-   [`cumsum()`](https://rdrr.io/r/base/cumsum.html) of base R is used to return a vector whose elements are the cumulative sums of the elements of the argument.\n\n-   [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.\n:::\n\n### **29.6.2 Removing lines with nested text representing quotes from other users.**\n\nIn this code chunk below, regular expressions are used to remove with nested text representing quotes from other users.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleaned_text <- cleaned_text %>%\n  filter(str_detect(text, \"^[^>]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article <\")\n  )\n```\n:::\n\n\n::: {.callout-note title=\"Things to learn from the code chunk:\"}\n-   [`str_detect()`](https://stringr.tidyverse.org/reference/str_detect.html) from **stringr** is used to detect the presence or absence of a pattern in a string.\n\n-   [`filter()`](https://dplyr.tidyverse.org/reference/filter.html) of **dplyr** package is used to subset a data frame, retaining all rows that satisfy the specified conditions.\n:::\n\n### **29.6.3 Text Data Processing**\n\nIn this code chunk below, [`unnest_tokens()`](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens, while [`stop_words()`](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words <- cleaned_text %>%\n  unnest_tokens(word, text) %>%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n```\n:::\n\n\nNow that we’ve removed the headers, signatures, and formatting, we can start exploring common words. For starters, we could find the most common words in the entire dataset, or within particular newsgroups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,542 × 2\n   word           n\n   <chr>      <int>\n 1 people        57\n 2 time          50\n 3 jesus         47\n 4 god           44\n 5 message       40\n 6 br            27\n 7 bible         23\n 8 drive         23\n 9 homosexual    23\n10 read          22\n# ℹ 5,532 more rows\n```\n\n\n:::\n:::\n\n\nInstead of counting individual word, you can also count words within by newsgroup by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_by_newsgroup <- usenet_words %>%\n  count(newsgroup, word, sort = TRUE) %>%\n  ungroup()\n```\n:::\n\n\n### **29.6.4 Visualising Words in newsgroups**\n\nIn this code chunk below, `wordcloud()` of **wordcloud** package is used to plot a static wordcloud.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex05_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::",
    "supporting": [
      "Hands-on_Ex05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}