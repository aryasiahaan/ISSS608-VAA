---
title: "Take-home Exercise 3"
author: "Arya Siahaan"
date: "May 15, 2024"
date-modified: "June 9, 2024"
execute: 
  eval: true
  echo: true
  warning: false
  freeze: true
---

# VAST Challenge 2024

### Mini-Challenge 2:

#### Background

In Oceanus, the island's economy thrives on the movement of seafaring vessels, particularly those operated by commercial fishing companies, indicating a healthy economic state. However, a major event has disrupted these routines: SouthSeafood Express Corp was caught engaging in illegal fishing. This scandal caused significant turmoil within the close-knit fishing community. FishEye International, a non-profit dedicated to combating illegal fishing, seeks assistance to understand the impact of this event. They have been gathering and processing data on ship movements and shipping records to create CatchNet: the Oceanus Knowledge Graph. While analysts at FishEye ensure data accuracy, they require assistance to develop analytical capabilities for this data.

#### Tasks and Questions:

FishEye analysts require assistance in performing geographic and temporal analysis of the CatchNet data to prevent future instances of illegal fishing. The task involves developing innovative visual analytics tools and workflows designed to identify and understand signatures of various behaviors. One key objective is to visualize the signature of SouthSeafood Express Corp's illegal activities. Additionally, there is a need to create a workflow capable of detecting other instances of illegal behavior within the data. These efforts are crucial for enhancing FishEye's ability to monitor and combat illegal fishing effectively. This exercise will attempt to address Question 1 and Question 3 of this challenge.

##### Question 1

FishEye analysts have long wanted to better understand the flow of commercially caught fish through Oceanus’s many ports. But as they were loading data into CatchNet, they discovered they had purchased the wrong port records. They wanted to get the ship off-load records, but they instead got the port-exit records (essentially trucks/trains leaving the port area). Port exit records do not include which vessel that delivered the products. Given this limitation, develop a visualization system to associate vessels with their probable cargos. Which vessels deliver which products and when? What are the seasonal trends and anomalies in the port exit records?

##### Question 3

To support further Fisheye investigations, develop visual analytics workflows that allow you to discover other vessels engaging in behaviors similar to SouthSeafood Express Corp’s illegal activities? Provide visual evidence of the similarities.

## Getting Started

### Loading the required R library packages

The following code chunk utilises the [`p_load()`](https://www.rdocumentation.org/packages/pacman/versions/0.5.1/topics/p_load) function from the [pacman](https://github.com/trinker/pacman) package to ensure that the necessary packages are available in the R environment. If the packages are already installed on the computer, [`p_load()`](https://www.rdocumentation.org/packages/pacman/versions/0.5.1/topics/p_load) will load them. If they are not installed, it will first install them and then load them into the R environment.

```{r}
pacman::p_load(tidyverse, jsonlite, igraph, 
               tidygraph, ggraph, SmartEDA,
               lubridate, ggplot2) 
```

### Loading The Data

The dataset is in the form of a json file and it is available for download at the [Vast Challenge 2024](https://vast-challenge.github.io/2024/index.html).

```{r}
mc2_data <- fromJSON("data/mc2.json")
```

## Data Preparation

### Wrangling and tidying edges

#### Extracting edges

Converts the links part of mc2_data to a tibble and removes duplicate rows, then displays the structure of the data. 

```{r}
# Wrangling and tidying edges
mc2_edges <- as_tibble(mc2_data$links) %>% distinct()
# Converts the date columns to POSIXct datetime format using the as_datetime function from the lubridate package, then display it with  glimpse() to confirm if the process have been performed correctly.
mc2_edges <- mc2_edges %>%
  mutate(date = as_datetime(date), time = as_datetime(time)) %>%
  mutate(date_only = as.Date(time))
glimpse(mc2_edges)
```




#### Splitting and Tidying the 'type' Column

This splits the type column into multiple columns (event2 and event3) and appends these columns back to the mc2_edges dataframe.
  
```{r}
word_list <- strsplit(mc2_edges$type, "\\.")
max_elements <- max(lengths(word_list)) #to find the maximum number of elements in any split
word_list_padded <- lapply(word_list, 
function(x) c(x, rep(NA, max_elements - length(x)))) #to pad shorter splits with NA values to make them all the same length.
word_df <- do.call(rbind, word_list_padded)
colnames(word_df) <- paste0("event", 1:max_elements)

# Since the output above is a matrix, the code chunk below is used to convert word_df into a tibble data.frame.
word_df <- as_tibble(word_df) %>%
  select(event2, event3)


# The code chunk below appends the extracted columns back to mc2_edges tibble data.frame
mc2_edges <- mc2_edges %>%
  cbind(word_df)
```



#### Handling unnecessary Columns

```{r}
# Dropping Unnecessary Columns
mc2_edges_cleaned <- mc2_edges %>%
  select(-c(`_last_edited_by`, `_date_added`, `_last_edited_date`, `_raw_source`, `_algorithm`, `key`, `type`, `data_author`, `aphorism`, `holiday_greeting`, `wisdom`, `saying of the sea`))

glimpse(mc2_edges_cleaned)
```


### Wrangling and tidying nodes

Extracting Nodes and Removing Duplicates

This converts the nodes data to a tibble and removes duplicate rows, then displays the structure of the data.

```{r}
mc2_nodes <- as_tibble(mc2_data$nodes) %>%
  distinct()
glimpse(mc2_nodes)
```


From the table above, beside the date data type and inappropriate field name issues we discussed earlier, two additional data issues can be observed. They are:

The values in Activities and fish_species_present fields are in list data type, which will affect the ability to process and to analyse the data. As shown in the screenshot below, some values in the Activities field are not ready to be analyse without further tidying (i.e. removing c(““)).

Tidying text field In the code chunk below, mutate() of dplyr and gsub() of Base R are used to perform the data tidying task.

This cleans up the Activities and fish_species_present columns by removing unnecessary characters.

```{r}
mc2_nodes <- mc2_nodes %>%
  mutate(Activities = gsub("c[(]", "", Activities)) %>% 
  mutate(Activities = gsub("\"", "", Activities)) %>%
  mutate(Activities = gsub("[)]", "", Activities)) 

mc2_nodes <- mc2_nodes %>%
  mutate(fish_species_present = gsub("c[(]", "", fish_species_present)) %>% 
  mutate(fish_species_present = gsub("\"", "", fish_species_present)) %>%
  mutate(fish_species_present = gsub("[)]", "", fish_species_present)) 
```


#### Dropping unnecessary Columns

```{r}
mc2_nodes_cleaned <- mc2_nodes %>%
  select(-c(`_last_edited_by`, `_date_added`, `_last_edited_date`, `_raw_source`, `_algorithm`, `style`))
glimpse(mc2_nodes_cleaned)
```



```{r}
# Creating subsets for nodes data
N_fish <- mc2_nodes_cleaned %>%
  filter(type == "Entity.Commodity.Fish") %>%
  select(-c(`type`, `Activities`, `fish_species_present`)) %>%
  rename(fish_species = name, fish_id = id)
```

```{r}
NL_City <- mc2_nodes_cleaned %>%
  filter(type == "Entity.Location.City") %>%
  select(-c(`type`, `fish_species_present`)) %>%
  rename(city_name = Name, city_id = id)
```

```{r}
NL_Point <- mc2_nodes_cleaned %>%
  filter(type == "Entity.Location.Point") %>%
  select(-c(`kind`, `fish_species_present`)) %>%
  rename(point_name = Name, point_id = id)
```

```{r}
NL_Region <- mc2_nodes_cleaned %>%
  filter(type == "Entity.Location.Region") %>%
  select(-c(`type`, `Description`)) %>%
  rename(region_name = Name, region_id = id, region_kind = kind)
```

```{r}
N_Delivery_doc <- mc2_nodes_cleaned %>%
  filter(type == "Entity.Document.DeliveryReport") %>%
  mutate(date = as.Date(date)) %>%
  rename(deliver_date = date, cargo_id = id) %>%
  select(-c(`type`, `Activities`, `fish_species_present`)) 
```

```{r}
glimpse(N_Delivery_doc)
```


```{r}
N_vessel <- mc2_nodes_cleaned %>%
  filter(grepl("Entity.Vessel", type)) %>%
  mutate(vessel_type = case_when(
    grepl("FishingVessel", type, ignore.case = TRUE) ~ "Fishing",
    grepl("Ferry.Passenger", type, ignore.case = TRUE) ~ "Ferry_Passenger",
    grepl("Ferry.Cargo", type, ignore.case = TRUE) ~ "Ferry_Cargo",
    grepl("Research", type, ignore.case = TRUE) ~ "Research", 
    grepl("Other", type, ignore.case = TRUE) ~ "Other", 
    grepl("Tour", type, ignore.case = TRUE) ~ "Tour", 
    grepl("CargoVessel", type, ignore.case = TRUE) ~ "Cargo_Vessel"
  )) %>%
  select(-c(`type`, `Activities`, `fish_species_present`)) %>%
  mutate(company = ifelse(is.na(company), "Unknown", company)) %>%
  rename(vessel_id = id, vessel_name = Name, vessel_company = company) 
```




```{r}
# Handling Transponder Pings
E_TransponderPing <- subset(mc2_edges_cleaned, event3 == "TransponderPing")
E_TransponderPing <- E_TransponderPing %>%
  rename(vessel_id = target) %>%
  left_join(N_vessel %>% select(vessel_id, vessel_type, vessel_company), by = "vessel_id") %>%
  filter(vessel_type == "Fishing") 



# Handling Transactions
E_Transaction <- subset(mc2_edges_cleaned, event2 == "Transaction")
E_Transaction <- E_Transaction %>%
  rename(cargo_id = source, 
         destination = target,
         transaction_date = date) %>%
  mutate(transaction_date = transaction_date - 1)
```


```{r}
glimpse(E_Transaction)
```




































# Extract Cargo to Fish relation data from Transaction Event data
Cargo_to_Fish <- E_Tx_c %>%
  filter(cargo_id %in% N_fish$fish_id) %>%
  select(cargo_id, fish_id = dest)

# Match Cargo ID in Delivery Documents to Fish ID from Transactions
N_Delivery_doc <- N_Delivery_doc %>%
  left_join(Cargo_to_Fish, by = "cargo_id") %>%
  left_join(N_fish %>% select(fish_id, fish_species), by = c("fish_id" = "fish_id"))

# Extract Cargo to Port relation data from Transaction Event data
Cargo_to_City <- E_Tx_c %>%
  filter(dest %in% NL_City$city_id) %>%
  select(cargo_id, port = dest)

# Match Cargo ID in Delivery Documents to Ports from Transactions
N_Delivery_doc <- N_Delivery_doc %>%
  left_join(Cargo_to_City, by = "cargo_id")

```


```
# Filter E_Tping_Fishing to include only rows with ping_source values that match port values
E_Tping_Fishing_filtered <- E_Tping_Fishing %>%
  filter(ping_source %in% unique(N_Delivery_doc$port))

# Verify the structure of E_Tping_Fishing_filtered to ensure correct filtering
glimpse(E_Tping_Fishing_filtered)

# Merge the datasets based on the date and port, acknowledging many-to-many relationship
Tping_to_Delivery <- E_Tping_Fishing_filtered %>%
  left_join(
    N_Delivery_doc %>% select(deliver_date, port, cargo_id, qty_tons, fish_id, fish_species), 
    by = c("date_only" = "deliver_date", "ping_source" = "port"),
    relationship = "many-to-many"
  )

# Verify the structure of Tping_to_Delivery to ensure the port column is included
glimpse(Tping_to_Delivery)

```


```
# Filter E_Tping_Fishing to include only rows with ping_source values that match port values
E_Tping_Fishing_filtered <- E_Tping_Fishing %>%
  filter(ping_source %in% unique(N_Delivery_doc$port))

# Verify the structure of E_Tping_Fishing_filtered to ensure correct filtering
glimpse(E_Tping_Fishing_filtered)

# Merge the datasets based on the date and port, acknowledging many-to-many relationship
Tping_to_Delivery <- E_Tping_Fishing_filtered %>%
  left_join(
    N_Delivery_doc %>% select(deliver_date, port, cargo_id, qty_tons, fish_id, fish_species), 
    by = c("date_only" = "deliver_date", "ping_source" = "port"),
    relationship = "many-to-many"
  )

# Verify the structure of Tping_to_Delivery
glimpse(Tping_to_Delivery)

```

```
# Verify the unique values in ping_source and port
unique_ping_source <- unique(E_Tping_Fishing$ping_source)
unique_port <- unique(N_Delivery_doc$port)

print(unique_ping_source)
print(unique_port)
```

```

# Merge the datasets based on the date and port, acknowledging many-to-many relationship
Tping_to_Delivery <- E_Tping_Fishing %>%
  filter(ping_source %in% NL_City$city_id) %>%
  left_join(
    N_Delivery_doc %>% select(deliver_date, port, cargo_id, qty_tons, fish_id, fish_species), 
    by = c("date_only" = "deliver_date", "ping_source" = "port"),
    relationship = "many-to-many"
  )

# Verify the structure of Tping_to_Delivery
glimpse(Tping_to_Delivery)


```





```

```
# Verify the structure of Tping_to_Delivery
glimpse(Tping_to_Delivery)
```

# Remove duplicate rows based on vessel_company and fish_species
distinct_species <- Tping_to_Delivery %>%
  distinct(vessel_company, fish_species) %>%
  na.omit() %>%
  select(vessel_company, fish_species)

# Match Cargo ID in Delivery Documents to Ports from Transactions
N_Delivery_doc <- N_Delivery_doc %>%
  left_join(Cargo_to_City, by = "cargo_id")
```

```
# Check the structure of Tping_to_Delivery
glimpse(Tping_to_Delivery)

```
```
# Visualize Vessel Activities by Port and Date
ggplot(vessel_activity, aes(x = date, y = total_qty_tons, color = fish_species)) +
  geom_line() +
  facet_wrap(~ port) +
  labs(title = "Vessel Activities by Port and Date", x = "Date", y = "Total Quantity of Fish (Tons)") +
  theme_minimal()
```

Subset the Data Based on Event Categories:
```

# Handling Transactions

# Subset Transactions Data
E_Tx <- subset(mc2_edges_cleaned, event2 == "Transaction")

# Transactions
E_Tx_c <- E_Tx %>%
  rename(
    cargo_id = source, 
    dest = target,
    tx_date = date) %>%
  mutate(tx_date = tx_date - 1) %>% # adjustment for records
  select(-c(time, dwell))
```

```
# Verify the content of the city_id column
unique(NL_City$city_id)
```


# Handling Harbor Reports

# Subset Harbor Report Data
E_HarborRpt <- subset(mc2_edges_cleaned, event2 == "HarborReport")

# Clean Harbor Report Data
E_Hbrpt_c <- E_HarborRpt %>%
  rename(vessel_id = source, port = target, arr_date = date)


# Handling Transponder Pings

# Subset Transponder Ping Data
E_TransponderPing <- subset(mc2_edges_cleaned, event3 == "TransponderPing")

# Clean Transponder Ping Data
E_Tping_c <- E_TransponderPing %>%
  rename(vessel_id = target, ping_source = source, start_time = time) 



```


Final Cleaned DataFrames
tx_c: Cleaned and joined transactions data, associating cargos with fish species.
E_Hbrpt_c: Cleaned harbor report data with relevant columns.
E_Tping_c: Cleaned transponder ping data with relevant columns.


```
N_fish <- mc2_nodes_cleaned %>%
  filter(type == "Entity.Commodity.Fish") %>%
  select_if(~ !any(is.na(.))) %>%
  select(-c(`type`, `Activities`, `fish_species_present`)) %>%
  rename(fish_species = name, fish_id = id)

N_Delivery_doc <- mc2_nodes_cleaned %>%
  filter(type == "Entity.Document.DeliveryReport") %>%
  select_if(~ !any(is.na(.))) %>%
  rename(deliver_date = date, cargo_id = id) %>%
  select(-c(`type`, `Activities`, `fish_species_present`)) %>%
  mutate(deliver_date = as.Date(deliver_date))


N_vessel <- mc2_nodes_cleaned %>%
  filter(grepl("Entity.Vessel", type)) %>%
  mutate(vessel_type = case_when(
    grepl("FishingVessel", type, ignore.case = TRUE) ~ "Fishing",
    grepl("Ferry.Passenger", type, ignore.case = TRUE) ~ "Ferry_Passenger",
    grepl("Ferry.Cargo", type, ignore.case = TRUE) ~ "Ferry_Cargo",
    grepl("Research", type, ignore.case = TRUE) ~ "Research", 
    grepl("Other", type, ignore.case = TRUE) ~ "Other", 
    grepl("Tour", type, ignore.case = TRUE) ~ "Tour", 
    grepl("CargoVessel", type, ignore.case = TRUE) ~ "Cargo_Vessel"
  )) %>%
  select(-c(`type`, `Activities`, `fish_species_present`)) %>%
  mutate(company = ifelse(is.na(company), "Unknown", company)) %>%
  rename(vessel_id = id, vessel_name = name, vessel_company = company) %>%
  select_if(~ !any(is.na(.)))

NL_City <- subset(mc2_nodes_cleaned, mc2_nodes_cleaned$type == "Entity.Location.City") %>%
  select_if(~ !any(is.na(.))) %>%
  select(-c(`type`, `fish_species_present`)) %>%
  rename(city_name = Name, city_id = id)

NL_Point <- subset(mc2_nodes_cleaned, mc2_nodes_cleaned$type == "Entity.Location.Point") %>%
  select_if(~ !any(is.na(.))) %>%
  select(-c(`kind`, `fish_species_present`)) %>%
  rename(point_name = Name, point_id = id)

NL_Region <- subset(mc2_nodes_cleaned, mc2_nodes_cleaned$type == "Entity.Location.Region") %>%
  select_if(~ !any(is.na(.))) %>%
  select(-c(`type`, `Description`)) %>%
  rename(region_name = Name, region_id = id, region_kind = kind)

```



```
# Join data tables to include vessel_type, vessel_company in transponder ping data and filter only Fishing vessel type
E_Tping_Fishing <- E_Tping_c %>%
  left_join(N_vessel %>% select(vessel_id, vessel_type, vessel_company), by = "vessel_id") %>%
  filter(vessel_type == "Fishing") %>%
  mutate(date_only = as.Date(start_time))

# Extract Cargo to Fish relation data from Transaction Event data
Cargo_to_Fish <- E_Tx_c %>%
  filter(cargo_id %in% N_fish$fish_id) %>%
  select(cargo_id, fish_id = dest)

# Match Cargo ID in Delivery Documents to Fish ID from Transactions
N_Delivery_doc <- N_Delivery_doc %>%
  left_join(Cargo_to_Fish, by = "cargo_id") %>%
  left_join(N_fish %>% select(fish_id, fish_species), by = c("fish_id" = "fish_id"))

# Extract Cargo to Port relation data from Transaction Event data
Cargo_to_City <- E_Tx_c %>%
  filter(dest %in% NL_City$city_id) %>%
  select(cargo_id, port = dest)

# Match Cargo ID in Delivery Documents to Ports from Transactions
N_Delivery_doc <- N_Delivery_doc %>%
  left_join(Cargo_to_City, by = "cargo_id")

# Merge the datasets based on the date and port
Tping_to_Delivery <- E_Tping_Fishing %>%
  filter(ping_source %in% NL_City$city_id) %>%
  left_join(N_Delivery_doc %>% select("deliver_date", "port", "cargo_id", "qty_tons", "fish_id", "fish_species"), by = c("date_only" = "deliver_date", "ping_source" = "port"))

# Remove duplicate rows based on vessel_company and fish_species
distinct_species <- Tping_to_Delivery %>%
  distinct(vessel_company, fish_species) %>%
  na.omit() %>%
  select(vessel_company, fish_species)
```





















