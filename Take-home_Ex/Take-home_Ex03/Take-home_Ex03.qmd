---
title: "Take-home Exercise 3"
author: "Arya Siahaan"
date: "May 15, 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  warning: false
  freeze: true
---

# VAST Challenge 2024

### Mini-Challenge 2:

#### Background

In Oceanus, the island's economy thrives on the movement of seafaring vessels, particularly those operated by commercial fishing companies, indicating a healthy economic state. However, a major event has disrupted these routines: SouthSeafood Express Corp was caught engaging in illegal fishing. This scandal caused significant turmoil within the close-knit fishing community. FishEye International, a non-profit dedicated to combating illegal fishing, seeks assistance to understand the impact of this event. They have been gathering and processing data on ship movements and shipping records to create CatchNet: the Oceanus Knowledge Graph. While analysts at FishEye ensure data accuracy, they require assistance to develop analytical capabilities for this data.

#### Tasks and Questions:

FishEye analysts require assistance in performing geographic and temporal analysis of the CatchNet data to prevent future instances of illegal fishing. The task involves developing innovative visual analytics tools and workflows designed to identify and understand signatures of various behaviors. One key objective is to visualize the signature of SouthSeafood Express Corp's illegal activities. Additionally, there is a need to create a workflow capable of detecting other instances of illegal behavior within the data. These efforts are crucial for enhancing FishEye's ability to monitor and combat illegal fishing effectively. This exercise will attempt to address Question 1 and Question 3 of this challenge.

##### Question 1

FishEye analysts have long wanted to better understand the flow of commercially caught fish through Oceanus’s many ports. But as they were loading data into CatchNet, they discovered they had purchased the wrong port records. They wanted to get the ship off-load records, but they instead got the port-exit records (essentially trucks/trains leaving the port area). Port exit records do not include which vessel that delivered the products. Given this limitation, develop a visualization system to associate vessels with their probable cargos. Which vessels deliver which products and when? What are the seasonal trends and anomalies in the port exit records?

##### Question 3

To support further Fisheye investigations, develop visual analytics workflows that allow you to discover other vessels engaging in behaviors similar to SouthSeafood Express Corp’s illegal activities? Provide visual evidence of the similarities.

## Getting Started

### Loading the required R library packages

For the purpose of this exercise, several R packages will be used to perform the following tasks:

-   Importing graph data in json file format into R

-   Extracting the nodes and edges (also known as links) from imported graph data.

-   Wrangling text data type.

-   Importing geographical data in geojson file format into R,

-   Importing geographical data in ESRI shapefile format into R,

-   Extracting movement data from the edges table of a knowledge graph,

-   Deriving a trajectory data from a movement data, and

-   Visualising trajectory data to reveal geo-temporal patterns.

The following code chunk utilises the [`p_load()`](https://www.rdocumentation.org/packages/pacman/versions/0.5.1/topics/p_load) function from the [pacman](https://github.com/trinker/pacman) package to ensure that the necessary packages are available in the R environment. If the packages are already installed on the computer, [`p_load()`](https://www.rdocumentation.org/packages/pacman/versions/0.5.1/topics/p_load) will load them. If they are not installed, it will first install them and then load them into the R environment.

```{r}
pacman::p_load(tidyverse, jsonlite, DataExplorer,
               lubridate, ggplot2, tidygraph,
               ggraph, igraph, sf, skimr) 
```

### Loading The Data

The dataset for this exercise is available for download at the [Vast Challenge 2024](https://vast-challenge.github.io/2024/index.html). As I undertake Mini-Challenge 2, I will utilize the dataset provided in the form of a JSON file named `mc2.json`.

In the code chunk below, `fromJSON()` of **jsonlite** package is used to import `mc2.json` file into R and save the output object.

```{r}
mc2_data <- fromJSON("data/mc2.json")
```

The output is called *`mc2_data`*. It is a large list R object.

## Data Preparation

### Wrangling and tidying *edges* dataframe

#### Extracting *edges* and removing duplicates

The code chunk below extracts the *links* dataframe from `mc2_data` and saves it as a tibble dataframe called `mc2_edges`. The `distinct()` function is used to remove duplicates.

```{r}
# Wrangling and tidying edges
mc2_edges <- as_tibble(mc2_data$links) %>% 
  distinct()
```

Next, `glimpse()` of **dplyr** package will be used to reveal the structure of `mc2_edges` tibble data table

```{r}
glimpse(mc2_edges)
```

::: {.callout-tip title="Observation"}
From the table above, the following data issues can be identified:

-   Columns with date and time data type are not in the correct format.

-   The column `type` also seems to contain three different types of information, namely: 'Event', 'TransportEvent', 'TransponderPing'.
:::

#### **Correcting time and date data type from mc2_edges_dataframe**

In the code chunk below, `as_datetime()` of **lubridate** package is used to convert fields with character date into **POSIXt** format.

```{r}
mc2_edges$time <- as_datetime(mc2_edges$time)
mc2_edges$"date" <- as_datetime("mc2_edges$date")
```

Next, `glimpse()` will be used again to confirm if the process have been performed correctly.

```{r}
glimpse(mc2_edges)
```

::: {.callout-tip title="Observation"}
As we can see now, the `time` and `date` columns have been converted to **datetime** format.
:::

#### **Splitting words**

![](images/clipboard-3479349286.png)

From the table above, we can see that the text in the `type` column is not tidy. I will tidy the `type` column by splitting it into three new separate columns called `event1`, `event2`, and `event3`. However, I will only keep the columns containing the values 'TransportEvent' and 'TransponderPing', which will be `event2` and `event3`.

#### Splitting and Tidying the 'type' Column from mc2_edges dataframe

```{r}
word_list <- strsplit(mc2_edges$type, "\\.")

# The code chunk below will be used to find the maximum number of elements in any split
max_elements <- max(lengths(word_list)) #to find the maximum number of elements in any split

#The code chunk below will be used to pad shorter splits with NA values to make them all the same length.
word_list_padded <- lapply(word_list, 
function(x) c(x, rep(NA, max_elements - length(x)))) #to pad shorter splits with NA values to make them all the same length.
word_df <- do.call(rbind, word_list_padded)
colnames(word_df) <- paste0("event", 1:max_elements)

# Since the output above is a matrix, the code chunk below is used to convert word_df into a tibble data.frame.
word_df <- as_tibble(word_df) %>%
  select(event2, event3)


# The code chunk below appends the extracted columns back to mc2_edges tibble data.frame
mc2_edges <- mc2_edges %>%
  cbind(word_df)
```

This code splits the `type` column into multiple components, pads shorter entries with NAs, and integrates them back into the `mc2_edges` dataframe as new columns.

![](images/clipboard-2806146328.png)

::: {.callout-tip title="Observation"}
As we can see here, two new columns, `event2` and `event3`, have been created and appended to the `mc2_edges` dataframe.
:::

### Initial EDA of mc2_edges dataframe

After cleaning and preparing the `mc2_edges` dataframe, I want to understand how the newly formed structure looks. Let's make use of the **DataExplorer** package to visualize it.

The code below uses the `plot_intro` function, which is part of the **DataExplorer** package. This function provides a high-level summary of the `mc2_edges` dataframe, offering insights into its structure and content.

::: panel-tabset
## The plot

![](images/clipboard-2055116904.png)

![](images/clipboard-1180112888.png)

## The code

```{r}
#| eval: false
# Plot introduction of the data
plot_intro(mc2_edges, title = "Introduction of mc2_edges Data")
```
:::

The `plot_str` function below generates a plot that visualizes the structure of `mc2_edges` dataframe, showing the relationships between different columns and their types.

::: panel-tabset
## The plot

![](images/clipboard-2619173514.png)

## The code

```{r}
#| eval: false
# Generate and display the data structure plot
plot_str(mc2_edges)

```
:::

The `plot_missing` function below generates a visualization that helps to identify and understand the distribution of missing values within `mc2_edges` dataframe.

::: panel-tabset
## The plot

![](images/clipboard-299140884.png)

## The code

```{r}
#| eval: false
# Plot missing values
plot_missing(mc2_edges, title = "Missing Values in mc2_edges Data")

```
:::

### Further cleaning of mc2_edges dataframe

After the initial EDA, I will remove columns from `mc2_edges` dataframe that are unnecessary to complete Mini-Challenge 2.

#### Dropping unnecessary columns from mc2_edges dataframe

```{r}
mc2_edges_cleaned <- mc2_edges %>%
  select(-c(`type`, `_last_edited_by`, `_date_added`, `_last_edited_date`, `_raw_source`, `_algorithm`, `key`,  `data_author`, `aphorism`, `holiday_greeting`, `wisdom`, `saying of the sea`))
```

The code above removes unnecessary columns from `mc2_edges` that are not needed for analysis, and then assigns the cleaned-up version to a new dataframe called `mc2_edges_cleaned`.

Next, let's use `glimpse()` to check what `mc2_edges_cleaned` looks like.

```{r}
glimpse(mc2_edges_cleaned)
```

Before moving to the next task, it would be wise to save the tidied `mc2_edges_cleaned` dataframe into a physical file for future use. By doing so, repeating the steps above will not be necessary.

The code chunk below will be used to save `mc2_edges_cleaned` into R **rds** file format.

::: callout-warning
Since the file will be saved in the `rds` sub-folder within the `data` folder, ensure that the `rds` folder exists. If it does not, it needs to be created first.
:::

```{r}
write_rds(mc2_edges_cleaned, "data/rds/mc2_edges_cleaned.rds")
```

This completes the data preparation process for the *links* dataframe of `mc2_data`, which is now stored in the `mc2_edges_cleaned` dataframe and saved as a physical `mc2_edges_cleaned.rds` file.

The next step is to clean and prepare the *nodes* dataframe of `mc2_data`.

### Wrangling and tidying *nodes* dataframe

#### Extracting *nodes* and removing duplicates

The code chunk below extracts the *nodes* dataframe from `mc2_data`, parses it as a tibble dataframe called `mc2_nodes`, and removes duplicate rows using the `distinct()` function.

```{r}
mc2_nodes <- as_tibble(mc2_data$nodes) %>%
  distinct()
```

Next, the code chunk below uses the `glimpse()` function to reveal the data structure of the `mc2_nodes` tibble dataframe.

```{r}
glimpse(mc2_nodes)
```

::: {.callout-tip title="Observation"}
From the table above, the following data issues can be identified:

-   The column `date` data type is not in the correct format.

-   The values in Activities and fish_species_present fields are in **list** data type, which will affect the ability to process and to analyse the data.

-   As shown in the screenshot below, some values in the Activities field are not ready to be analyse without further tidying (i.e. removing c(““)).
:::

#### **Correcting date data type from mc2_nodes dataframe**

In the code chunk below, `as_datetime()` of **lubridate** package is used to convert date fields with character date into **POSIXt** format.

```{r}
mc2_nodes$date <- as_datetime(mc2_nodes$date)
```

Next, `glimpse()` will be used again to confirm if the process have been performed correctly.

```{r}
glimpse(mc2_nodes)
```

::: {.callout-tip title="Observation"}
As we can see now, the time and date columns have been converted to datetime format.
:::

There are two more additional data issues can be observed. They are:

The values in `Activities` and `fish_species_present` columns are in list data type, which will affect the ability to process and to analyze the data.

As shown in the screenshot below, some values in the `Activities` and `fish_species_present`column are not ready to be analyze without further tidying (i.e. removing c(““)).

![](images/clipboard-2425530968.png)

#### Tidying Text Field

In the code chunk below, `mutate()` from `dplyr` and `gsub()` from Base R are used to perform the data tidying task.

This cleans up the `Activities` and `fish_species_present` columns by removing unnecessary characters.

```{r}

mc2_nodes <- mc2_nodes %>%
  mutate(Activities = gsub("c[(]", "", Activities)) %>% 
  mutate(Activities = gsub("\"", "", Activities)) %>%
  mutate(Activities = gsub("[)]", "", Activities)) 

mc2_nodes <- mc2_nodes %>%
  mutate(fish_species_present = gsub("c[(]", "", fish_species_present)) %>% 
  mutate(fish_species_present = gsub("\"", "", fish_species_present)) %>%
  mutate(fish_species_present = gsub("[)]", "", fish_species_present)) 
```

After executing the code chunk above, we can see from the screenshot below, that the text field in `Activities` and `fish_species_present` columns have been tidied.

![](images/clipboard-3517877848.png)

### **INITIAL EDA of mc2_nodes dataframe**

Now, let's use the DataExplorer package again to visualize the `mc2_nodes` dataframe in a similar manner as was done above for the `mc2_edges` dataframe.

::: panel-tabset
## The plot

![](images/clipboard-4088556349.png)

![](images/clipboard-290209426.png)

## The code

```{r}
#| eval: false
# Plot introduction of the data
plot_intro(mc2_nodes, title = "Introduction of mc2_nodes Data")
```
:::

The `plot_str` function below generates a plot that visualizes the structure of `mc2_nodes` dataframe, showing the relationships between different columns and their types.

::: panel-tabset
## The plot

![](images/clipboard-1740825281.png)

## The code

```{r}
#| eval: false
# Generate and display the data structure plot
plot_str(mc2_nodes)

```
:::

The `plot_missing` function below generates a visualization that helps to identify and understand the distribution of missing values within `mc2_nodes` dataframe.

::: panel-tabset
## The plot

![](images/clipboard-1008176763.png)

## The code

```{r}
#| eval: false
# Plot missing values
plot_missing(mc2_nodes, title = "Missing Values in mc2_nodes Data")

```
:::

### **FURTHER CLEANING of mc2_nodes dataframe**

After the initial EDA, I will remove columns from `mc2_nodes` dataframe that are unnecessary to complete Mini-Challenge 2.

#### Dropping unnecessary Columns from mc2_nodes dataframe

```{r}
mc2_nodes_cleaned <- mc2_nodes %>%
  select(-c(`_last_edited_by`, `_date_added`, `_last_edited_date`, `_raw_source`, `_algorithm`, `style`))
```

The code above removes unnecessary columns from `mc2_nodes` that are not needed for analysis, and then assigns the cleaned-up version to a new dataframe called `mc2_nodes_cleaned`.

Next, let’s use `glimpse()` to check what `mc2_nodes_cleaned` looks like.

```{r}
glimpse(mc2_nodes_cleaned)
```

The tidied mc2_nodes_cleaned dataframe will then be saved into a physical file for future use. By doing so, repeating the steps above will not be necessary.

The code chunk below will be used to save mc2_nodes_cleaned into R rds file format.

```{r}
write_rds(mc2_nodes_cleaned, "data/rds/mc2_nodes_cleaned.rds")
```

This completes the data preparation process for the *nodes* dataframe of `mc2_data`, which is now stored in the `mc2_nodes_cleaned` dataframe and saved as a physical `mc2_nodes_cleaned.rds` file.

### Importing Geographical Data in geojson format

Next I need to import geographical data. The data is in the form of GEOJSON file. GEOJSON is an open standard format designed for representing simple geographical features, along with their non-spatial attributes. It is based on the JSON format. I will import this using the `sf` package.

In the code chunk below, `st_read()` of **sf** package is used to import a geographical file in [geojson](https://en.wikipedia.org/wiki/GeoJSON) format into R.

```{r}
oceanusgeography = st_read("data/OceanusGeography.geojson") %>%
  st_transform(crs = 4326)
```

::: {.callout-note title="Thing to learn from the code chunk above"}
`st_transform()` is used to assign wgs84 (i.e. 4326) coordinates system to the output object.
:::

The output R object (i.e. oceanusgeography) is an sf data.frame as shown below. Actually sf stand for simple feature.

```{r}
class(oceanusgeography)
```

Next, `glimpse()` is used to display the structure of *oceanusgeography* sf data.frame.

```{r}
glimpse(oceanusgeography)
```

::: callout-note
The geometry column indicate the simple feature geometry is multipolygon. However, if the dataframe is examined closely, we can see that the geographical data are captured in polygon and point features.
:::

#### Visualising Geographical Data with ggplot2

It is always a good practice to visualize the sf data.frame in a map.

In the code chunk below, geom_sf() of ggplot2 package is used to plot the sf data.frame in R.

```{r}
ggplot(data = oceanusgeography) +
  geom_sf()
```

Before moving on to next section, let’s save oceanusgeography into rds format for future use by using the code chunk below.

```{r}
write_rds(oceanusgeography, "data/rds/oceanusgeography.rds")
```

This completes the data preparation process for the *`oceanusgeography`* dataframe, which is now stored in the `oceanusgeography` dataframe and saved as a physical `oceanusgeography.rds` file.

#### Importing Geographical Data in ESRI shapefile format

In the code chunk below, `st_read()` of **sf** package is used to import a geographical file in [ESRI shapefile](#0) format into R.

```{r}
oceanuslocations <- st_read(dsn = "data/shp",
  layer = "Oceanus Geography")
```

Next `class()` is used verify if the *OceanusLocations* is indeed in sf data.frame.

```{r}
class(oceanuslocations)
```

Similarly, `glimpse()` is used to reveal the structure of *OceanusLocation* data.frame.

```{r}
glimpse(oceanuslocations)
```

Let's visualize the geographical data by using `geom_sf()` of ggplot2 as shown in the code chunk below.

```{r}
ggplot(data = oceanuslocations) +
  geom_sf()
```

Before moving on to the next section, the code chunk below is used to save *oceanuslocations* into an rds file format.

```{r}
write_rds(oceanuslocations, 
  "data/rds/oceanuslocations.rds")
```

This completes the data preparation process for the *oceanuslocations*, which is now stored in the `oceanuslocations` dataframe and saved as a physical `oceanuslocations.rds` file.

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

## Data Analysis

After completing the data preparation process, the next step is to perform the analysis required to complete Mini-Challenge 2. From the background information above, we know that "SouthSeafood Express Corp" has been caught fishing illegally. Now, let's see what we can discover from the available data regarding their illegal behavior.

#### Importing Edges Data

Now it is time to bring mc2_edges_cleaned.rds into R

```{r}
mc2_edges_cleaned <- read_rds("data/rds/mc2_edges_cleaned.rds")
```

Next, `unique()` is used to list the unique classes in *event3* column of *mc2_edges* data.frame.

```{r}
unique(mc2_edges$event3)
```

The output above shows that there are two unique classes in the event3. They are: *TransponderPing* and NA.

#### Extract Vessel Movement Data

The code chunk below is used to extract vessel movement data from mc2_edges.

::: {.callout-note title="Thing to learn from the code chunk above"}
-   `filter()` is used to select records with event3 is TransponderPing, and

-   `select()` is used to fields that are needed to be saved
:::

```{r}
vessel_movement_data <- mc2_edges %>%
  filter(event3 == "TransponderPing") %>%
  select(time, dwell, source, target)
```

```{r}

```

```{r}

```

```{r}

```

#### Investigate 'type' column from the nodes data

Next, I will use the `unique()` function from the base R package to see all the distinct entries in the `type` column of the `mc2_nodes_cleaned` dataframe.

```{r}
unique(mc2_nodes_cleaned$type)
```

::: {.callout-tip title="Observation"}
The `type` column from the nodes data seems to contain a mix of information. Some of the entries appear interesting and useful for analysis, including:

-   Entity.Commodity.Fish

-   Entity.Document.DeliveryReport

-   Entity.Location.City

-   Entity.Location.Point

-   Entity.Location.Region

-   Entity.Vessel.CargoVessel

-   Entity.Vessel.FishingVessel

The other entries seem useless.
:::

Next, I will try to find out the count of each entry. The code chunk below allows me to see the count of unique value in each entry.

```{r}
nodes_type_counts <- mc2_nodes_cleaned %>%
  group_by(type) %>%
  summarise(count = n())

# Display the result
print(nodes_type_counts)
```

::: {.callout-tip title="Observation"}
Based on the table above, we can see that there are:

-   10 Fish Types.

-   5307 Delivery Reports.

-   6 City Names

-   12 Point Locations

-   6 Region Locations

-   100 Cargo Vessels.

-   178 Fishing Vessels.
:::

```{r}
# Filter fishing vessels and rename specific columns while keeping all others
fishing_vessel <- mc2_nodes_cleaned %>%
  filter(type %in% c("Entity.Vessel.FishingVessel")) %>%
  rename(
    fishing_vessel_id = id,
    fishing_vessel_name = Name
  )
```

```{r}
glimpse(fishing_vessel)
```

After creating a new dataframe called fishing_vessel to isolate fishing vessel information, I want to find out which of these fishing vessels are registered under "SouthSeafood Express Corp".

```{r}
# Filter fishing vessels registered under "SouthSeafood Express Corp"
southseafood_fishing_vessels <- fishing_vessel %>%
  filter(company == "SouthSeafood Express Corp")

# Display the result
print(southseafood_fishing_vessels)
```

::: {.callout-tip title="Observation"}
The fishing vessels belonging to "SouthSeafood Express Corp" are:

-   Snapper Snatcher (`snappersnatcher7be`)

-   Roach Robber (`roachrobberdb6`)
:::

### Further Data Subsetting and Renaming

#### Creating Subset Data

Let's try to create a new dataframe from the mc2_edges_cleaned dataframe, filtering only the transponderping information

```{r}
transponderping <- subset(mc2_edges_cleaned, event3 == "TransponderPing")
```

```{r}
unique(transponderping$source)
```

The unique values contained in the source column from transponderping dataframe seems to contain information regarding fishing grounds and ports

```{r}
#unique(transponderping$target)
```

The target column seems to contain fishing vessel ID

```{r}
unique(mc2_nodes_cleaned$kind)
```

```{r}
unique(mc2_nodes_cleaned$kind)
```

```{r}
fishing_ground <- subset(mc2_nodes_cleaned, kind == "Fishing Ground")
```

Creates subsets of data for different types of events: transponder pings, transactions, and harbor reports, facilitating focused analysis.

```{r}
transponderping <- subset(mc2_edges_cleaned, event3 == "TransponderPing")
transaction <- subset(mc2_edges_cleaned, event2 == "Transaction")
harbor_report <- subset(mc2_edges_cleaned,  event2 == "HarborReport")
```

```{r}
mc2_nodes_type_counts <- mc2_nodes %>%
  group_by(type) %>%
  summarise(count = n())

# Display the result
print(mc2_nodes_type_counts)
```

```{r}
mc2_nodes_name_counts <- mc2_nodes %>%
  group_by(name) %>%
  summarise(count = n())

# Display the result
print(mc2_nodes_name_counts)
```

```{r}

```

```{r}

```

Now I want to find out all the companies that own these fishing vessels

```{r}
#unique(fishing_vessel$fishing_vessel_name)
```

```{r}
# Filter fishing vessels and rename specific columns while keeping all others
fish_species <- mc2_nodes_cleaned %>%
  filter(type %in% c("Entity.Commodity.Fish")) %>%
  rename(
    fish_name = name,
    fish_id = id
  )
```

Continue from here

Now I want to find out all the companies that own these fishing vessels

```{r}
unique(fish_species$fish_name)
```

From the result above I see "SouthSeafood Express Corp" is listed, now I want to isolate all the fishing vessels belong to "SouthSeafood Express Corp".

```{r}
# Filter fishing vessels owned by "SouthSeafood Express Corp"
southseafood_fishing_vessels <- fishing_vessel %>%
  filter(company == "SouthSeafood Express Corp")
print(southseafood_fishing_vessels)
```

So the vessels belong to "SouthSeafood Express Corp" are

-   Snapper Snatcher (snappersnatcher7be)

-   Roach Robber (roachrobberdb6)

After identifying the name of the fishing vessels belong to "SouthSeafood Express Corp", I need to check whether this information is also contained in the mc2_edges_cleaned dataframe.

```{r}
# Check if specific values are in the source column
values_exist <- c("snappersnatcher7be", "roachrobberdb6") %in% mc2_edges_cleaned$source

# Print results
print(values_exist)

```

The output \[1\] TRUE TRUE indicates that both identifiers "snappersnatcher7be" and "roachrobberdb6" are present in the source column of mc2_edges_cleaned dataframe.

The next steps will involve deeper analysis and visualization to understand the activities and patterns associated with these vessels.

```{r}
# Extract relevant data for the specified vessels
southseafood_activities <- mc2_edges_cleaned %>%
  filter(source %in% c("snappersnatcher7be", "roachrobberdb6")) 

```

```{r}
glimpse(southseafood_activities)
```

```{r}
harbor_report <- mc2_edges_cleaned %>%
  filter(event2 == "HarborReport")
```

```{r}
#unique(mc2_edges_cleaned$source)
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}
# Filter cargo vessels and select specific columns with renaming
cargo_vessel <- mc2_nodes_cleaned %>%
  filter(type %in% c("Entity.Vessel.CargoVessel")) %>%
  select(
    vessel_type = type,
    cargo_vessel_id = id,
    cargo_vessel_name = Name,
    company
  )
```

```{r}
# unique(fishing_vessel$company)
```

```{r}

```

```{r}

```

```{r}
OceanusGeography = st_read("data/OceanusGeography.geojson") %>%
  st_transform(crs = 4326)
```

```{r}
class(OceanusGeography)
```

```{r}
glimpse(OceanusGeography)
```

```{r}
ggplot(data = OceanusGeography) +
  geom_sf()
```

```{r}
write_rds(OceanusGeography, "data/rds/OceanusGeography.rds")
```

```{r}
OceanusLocations <- st_read(dsn = "data/shp",
  layer = "Oceanus Geography")
```

```{r}
class(OceanusLocations)
```

```{r}
glimpse(OceanusLocations)
```

```{r}
ggplot(data = OceanusLocations) +
  geom_sf()
```

```{r}
write_rds(OceanusLocations, 
  "data/rds/OceanusLocations.rds")
```

```{r}
vessel_movement_data <- mc2_edges %>%
  filter(event3 == "TransponderPing") %>%
  select(time, dwell, source, target)
```

```{r}
unique(vessel_movement_data$source)
```

```{r}
unique(OceanusLocations$Name)
```

```{r}
vessel_movement_data <- vessel_movement_data %>%
  mutate(source = gsub("^City of", "", source)) %>%
  mutate(source = gsub("^\\s+", "", source))
```

```{r}
unique(vessel_movement_data$source)
```

```{r}
coords <- st_coordinates(OceanusLocations)
```

```{r}
OceanusLocations_df <- OceanusLocations %>%
  st_drop_geometry()
```

```{r}
OceanusLocations_df$XCOORD <- coords[, "X"]
OceanusLocations_df$YCOORD <- coords[, "Y"]
```

```{r}
class(OceanusLocations_df)
```

```{r}
OceanusLocations_df <- OceanusLocations_df %>%
  select(Name, X.Kind, XCOORD, YCOORD) %>%
  rename(Loc_Type = X.Kind)
```

```{r}
vessel_movement_data <- vessel_movement_data %>%
  left_join(OceanusLocations_df,
            by = c("source" = "Name"))
```

```{r}
write_rds(vessel_movement_data, "data/rds/vessel_movement_data.rds")
```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
